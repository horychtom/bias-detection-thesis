\chapter{Experiments}\label{experiments}
I follow current sota approaches which is using large pretrained language models.
\section{Czech models}
RobeCzech
\begin{enumerate}
    \item RobeCZECH
    \item Czert
    \item FERNET-C5
    \item FERNET-News
\end{enumerate}

%multilingual
\begin{enumerate}
    \item SlavicBert
    \item xlm-roberta-large
    \item mBERT
\end{enumerate}
zm√≠nka RCI cluster
\begin{enumerate}
    \item Training args tuning
    \item Weights and biases visualisations
    \item Pre-finetuning, self-training
    \item experiment on collective dataset
\end{enumerate}

\section{Evaluation}
For the BABE dataset baseline, the 15\% of data has been saved for final evluation. I decided not to use early stopping, since cross validation ahs been used and it doesnt go well together
\section{Inference on Czech News Samples}
\subsection{Analysis and statistics}
\subsection{Few words Article level}

\section{LIME analysis and demo}

\section{Multi-Task learning approach}\label{mtl}