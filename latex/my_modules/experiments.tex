\chapter{Experiments}\label{experiments}
To complete the last research task proposed in the introduction \hyperref[problem_definition]{T4}, firstly I perform experiments on collected datasets to answer the two main research qusetions \hyperref[Q1]{Q1} and \hyperref[Q2]{Q2}. The main \textbf{target} dataset for evaluations is the BABE dataset due to its high quality and properties. Then using the optimal dataset combination and hyperparameters, a final Czech classifier is built and used for inference.

Because of novelty of the CWNC I also perform a baseline evaluation on this dataset, but furthermore it is not tuned nor evaluated.
I follow the current standard approaches and use pre-trained transformers for further \textbf{pre-training} and \textbf{fine-tuning}.

To train a model for the classification of \gls{mb}, a pre-trained transformer language model is used. Trained solely on Czech data or on multiple languages jointly. Then a classiffier consisting of a dense\footnote{Often referred to as fully-connected layer.} layer is attached to the model output to perform the binary classification task. A scheme of the architecture used can be seen in \ref{fig:classifier}.

\begin{figure}
\makebox[\textwidth][c]{
  \includegraphics[scale=0.3]{my_modules/multimedia/final_colored.png}
  \caption{Scheme of a text classification architecture used in fine-tuning process.}
  \label{fig:classifier}
  }
\end{figure}

In this context, \textbf{fine-tuning} is the process of adapting the pre-trained language model as well as the classification head, to the BABE dataset. Both the parameters of the language model, e.g. BERT, and the parameters of the classifier are updated jointly. Weights of the classification layer are initialized randomly and trained from scratch; however, pre-trained language model already contains meaningful linguistic features so it's parameters are \textbf{fine-tuned} for bias-specific representations. This is especially convenient since the BABE dataset has only around 3700 sentences. Therefore, training from scratch would likely fail to learn general linguistic properties together with subtle bias features.

I also experiment with further \textbf{pre-training} on in-domain datasets. This means that before fine-tuning, an extra step is performed. In this step, the model with the architecture mentioned above is further \textbf{pre-trained} on one or more bias datasets from the collection of Czech datasets presented in this work. This secondary pre-training aims to learn meaningful \textbf{bias representations} and therefore help with subsequent fine-tuning on the BABE.




As opposed to English language, there is a relatively low number of Czech pre-trained transformer models available. A list and a brief summary of all the models tested in the experiments can be found in the following:




\subsection{Czech monolingual models}
Models trained solely on Czech data.
\begin{itemize}
    \item \textbf{RobeCzech} \cite{strakarobeczech} - RoBERTa-based model with 125M parameters. Like its original counterpart, it is trained with the \gls{mlm} task, on 4,917M tokens of Czech corpora.
    \item \textbf{Czert} \cite{sido-etal-2021-czert} - BERT-based model with 110M parameters, trained with \gls{nsp} tasks. All Together trained on 37GB of text. 
    \item \textbf{FERNET-C5} \cite{lehevcka2021comparison} - BERT-based model trained with the \gls{mlm} and \gls{nsp} task on 93GB of text from the Common Crawl project.
    \item \textbf{FERNET-News} \cite{lehevcka2021comparison} - RoBERTa-based model trained with \gls{mlm} task on 20GB of Czech News text.
\end{itemize}




\subsection{Multi-lingual models}
Models trained jointly on multiple languages.
\begin{itemize}
    \item \textbf{SlavicBert} \cite{arkhipov2019tuning} - BERT-based model with 179M parameters, trained on four languages: Russian, Bulgarian, Czech, and Polish. The model is trained on all 4 languages at once. The model is not trained from scratch, but it is a fine-tuned version of mBERT.
    \item \textbf{mBERT} \cite{devlin2019bert} - BERT-based model with 179M parameters trained on corpora of 104 languages, including Czech, with MLM task.
\end{itemize}




\section{Experimental setup}
All models are fetched, trained, and evaluated using the HuggingFace API\footnote{\url{https://huggingface.co/docs}}. The maximum sequence length is set to 128 tokens. All parameters can be seen in the Appendix \ref{all_parameters}.

A small portion (15\%) of the target dataset is left aside as a \textbf{test set} at the beginning and is used only for the final evaluation to ensure that no test data leak into the training data.

Every evaluation, except the one performed on the final test set, is done using a 10-fold \gls{cv}. This helps to get more realistic estimates of model performance than a simple train-validation split would give. The only evaluation metric used for all experiments is the F1 score with \textit{macro} averaging\footnote{The F1 score is computed for both classes and averaged.}. 

All training has been done on the RCI cluster node with 4 x NVIDIA Tesla V100 with 32GB GPU graphic memory.





 \section{Baseline setup}
 As a baseline, all Czech and multilingual models are fine-tuned on BABE and evaluated using a 10-fold stratified \gls{cv}. The hyperparameters used are the same as those used by the authors of BABE \cite{Spinde2021MBIC}. However, the authors used early stopping together with \gls{cv} and used the validation split inside \gls{cv} to early stop, which allows the model to "see" the test data during training before the evaluation. This condition may lead to too optimistic results.
 
 A solution to this problem would require another split for validation, but at this point, the size of the training data is already shrunk significantly. Therefore, I did not use early stopping together with \gls{cv} at all. Instead, I fixed the number of epochs to 3 as suggested by the authors of BERT \cite{devlin2019bert} . 
 All other hyperaparemeters remained unchanged; AdamW optimizer is used with an initial learning rate of 5e-5 and a batch size of 64.
 
 The baseline evaluation of all Czech models used can be seen in table \ref{table:3}. The final F1 score is averaged across all folds.
 
 The model that performs best on the BABE is \textbf{FERNET-C5}. It also performs best on the novel CWNC dataset; therefore, it is a suitable candidate for further tuning. From now on, all experiments are performed using this model.
 

 \input{my_modules/tables/baseline}

 
 
 
 
\newpage

 \section{Hyperparameter tuning}
I restricted the search space only to the combinations of:
 \begin{itemize}
     \item \textbf{Batch size} $\in \{16,32\}$
     \item \textbf{Learning rate} $\in $ \{2e-5,3e-5,5e-5\}
     \item \textbf{Epochs} $\in \{2,3,4\}$
 \end{itemize}
 
 As the authors of the original BERT paper suggest. Then I ran a grid search with \gls{cv}. The overall best parameters were as follows:
 \begin{center}
      \{learning\_rate = 3e-5,batch\_size = 32, epochs=3\}\label{hyperparams}
 \end{center}
 
 The model with the best parameters achieved a 0.784 F1 score ($\sim$0.4\% improvement against baseline).


 
 
\input{my_modules/tables/all_trained}





\section{Combining Datasets}
This section is dedicated to the study of the influence of pre-training on combinations of datasets. Trying all combinations would result in training 511 models\footnote{Given a set of $n$ elements, number of subsets is $2^n$. Here we have a set of 9 datasets resulting in 512 combinations. 511 without an empty set.}, which is infeasible for obvious reasons. Therefore, I decided to experiment with pre-training on five sets of datasets with regard to their bias information, to see which of them can serve as a good initialization for fine-tuning on BABE.
The combination sets are as follows:
\begin{itemize}
    \item \textbf{SUBJe} - is a combination of SUBJ and MPQA dataset, which both focus on explicit subjective bias.
    \item \textbf{MB} - is a combination of NJNJ, UA-crisis and BASIL dataset which are all from the \gls{mb} family.
    \item \textbf{WIKI} - are all datasets collected from Wikipedia. It consists of CW-hard, WikiBias, and CWNC. The three datasets were collected automatically with respect to NPOV violations as described in \ref{wiki-npov}.
    \item \textbf{ALL} - This one is simply a combination of all datasets except the WNC.
    \item \textbf{WNC} - WNC is almost 90\% of all data; therefore, I perform experiments on this dataset separately.
\end{itemize}
 
In every combination, data were randomly mixed and subsequently downsampled, so that classes were balanced. For each combination, 20\% of data were used as a validation set to decide the optimal number of epochs for pre-training. The convergence of validation losses can be seen in the figure \ref{fig:all_losses}. The number of epochs for pretraining were chosen as follows: 1, 3, 1, 2, 1 for SUBJe, MB, WIKI, ALL and WNC respectively.
\begin{figure}
  \includegraphics[scale=0.5]{my_modules/multimedia/all_losses.png}
  \caption{Convergence of validation loss over different dataset combinations}
  \label{fig:all_losses}
\end{figure}


\subsection{Pre-training + Evaluating}
Firstly, pre-trained models are evaluated on BABE without any further training on it. This way, it can be studied how well each model trained on each set can transfer knowledge to detection of \gls{mb} in BABE. This possibly unveils the relatedness to BABE. The results can be seen in table \ref{table:4}. In the table, this pre-training \textbf{without} further fine-tuning is referred to as\textbf{Pre-Training + Evaluating}.

There is clear evidence that models pre-trained on Wikipedia data, both \textbf{WIKI} and \textbf{WNC} perform relatively well compared to \textbf{MB} and \textbf{SUBJ}. This suggests that the bias distribution in WIKI datasets is the closest to BABE.

During pre-training, the F1 score of the WIKI and the MB set both peaked around 70\% on validation set; however, the model trained on MB generalized very poorly to BABE data as opposed to the WIKI model (0.46 against 0.67). I suspect that this result is due to the high size imbalance between the two sets.


\subsection{Pre-training + Fine-tuning}
Secondly, pre-trained models are used as a weight initialization for fine-tuning on BABE dataset.
Pre-training on \textbf{ALL} datasets combined resulted in the best performance; however, virtually the same performance was achieved by the model trained on the \textbf{SUBJe} combination. The difference is statistically insignificant. Importantly, the SUBJ split represents almost half of all the data (see \ref{fig:cz_data}). Therefore, I assume that the performance of ALL split is high due to the presence of SUBJe.

Pre-training on the other sets hurts the performance. Yet, the difference is very small. The lowest score is achieved by pre-training on MB set. I suspect this is mainly because the small size (2500 sentences) of the balanced MB set may have led to overfitting.


The subjectivity bias as opposed to the media bias appears to be a bit more explicit and more straightforward. Therefore, it can help classify \gls{mb} but not the other way round since the pre-trained SUBJe model achieved only 55\% on BABE. This supports the assumption that \gls{mb} is composed of many more superficial linguistic features \ref{features}. However, these results are still greatly influenced by the overall small size and low quality of the datasets.




\section{Results}
The final model for evaluation has been pre-trained with \textbf{ALL} datasets combination and fine-tuned with optimal parameters (\ref{hyperparams}) on BABE target dataset. Finally, on a \textbf{test} set it achieved an F1 score of \textbf{0.804}. 

Afterward, for the final model that I share\footnote{\url{https://huggingface.co/horychtom/czech_media_bias_classifier}} on huggingface and which is used for inference experiments, the entire BABE dataset, including the test set, is used for the training.

\section{Discussion}
Despite the relatively high performance, the final score in the test set is not representative due to its size. The test set consists of $\sim$ 500 sentences and therefore, may not adequately represent all bias information. For better evaluation, I propose using a nested \gls{cv}. However, the computational demand would be very high. 

Possibly, a complete study with more models could be performed, but that would require an enormous number of trained models. Essentially, these results show that there was a minimal gain over the baseline (\textbf{+0.7\%}).

The authors of the original paper that introduces BABE \cite{spinde2021neural} report an F1 score of $\sim$ 0.8 for fine-tuned transformer model. That is approximately 2\% higher score than I achieved on the Czech version. This may be caused either by the noise introduced into the data during machine translation, or by the possibly lower quality of the Czech pre-trained models.





\newpage

\section{Inference on Czech News Samples}\label{inference}

\begin{itemize}
    \item abstract,headline, text bias correlation
    \item vbias progress over time for one domain
    \item plot the difference between domains
    \item plot the difference between topics
    \item info about 'commentary' articles is presented! super
\end{itemize}

\subsection{Demo}
Additionally, I provide a simple web demo for the reader to experiment with\footnote{\url{https://huggingface.co/spaces/horychtom/czech_media_bias_detection}}. The use of the demo can be seen in \ref{fig:demodemo}. The backend runs on HuggingFace's spaces\footnote{\url{https://huggingface.co/spaces}} which is a free hosting service for demonstration of \gls{ml} applications. 

The user can insert arbitrary text in Czech language; text is then split into sentences and classified individually. Then the average percentage bias score of the text is displayed. An \textit{interpret} button allows the user to further inspect the classification.

\begin{figure}
\makebox[\textwidth][c]{
  \includegraphics[scale=0.3]{my_modules/multimedia/bias.jpg}
  \caption{Example of the bias classifier demo usage. Red and blue color represent the biased and unbiased annotation respectively.}
  \label{fig:demodemo}
  }
\end{figure}
