\chapter{Experiments}\label{experiments}
In this section I present experiments on text classification over colelcted datasets. Main target dataset for evaluations is BABE, because of its high quality and properties.

Because of novelty of CWNC I also perform evaluation on this dataset.
I follow the current standard approaches and use pretrained transformers for further pretraining and fine-tuning. One possibility is to use multilingual models, which are trained on set of languages to capture general language properties. In recent years, there have also purely czech models emerged.
\subsection{Czech models}
\begin{itemize}
    \item RobeCZECH
    \item Czert
    \item FERNET-C5
    \item FERNET-News
\end{itemize}


\subsection{Multi-lingual models}
\begin{itemize}
    \item SlavicBert
    \item mBERT
\end{itemize}


 15\% of BABE data has been saved for final evaluation. Steps are as follows:
 \begin{itemize}
     \item BASELINE:
	5-CV plain model finetuning on BABE (could be split to train validation but we dont have that many data)
	\item HYPERPARAM-TUNING:
	5-CV model tuning parameters (grid search) on BABE
	\item DATA-COMBINATIONS:
	5-CV model with tuned hyperparams with varying combinations of data on BABE
	\item EVALUATION:
	pick the best model, train it with params and early stopping, and run it on - BABE test set 
										       - CWNC test set
	early stopping only on final training! Because the authors essentially made early stopping on "testing set" + it is not recommended to use early stopping in cross validation
 \end{itemize}
 
 
 
 
 
All training has been done on a single GPU on RCI cluster.
 \section{Baseline setup}
 As a baseline, I finetuned all Czech models listed above on BABE and evaluated them using 5-fold stratified cross validation. Hyperparameters were the same as used by authors \cite{Spinde2021MBIC}. However, the authors early stopping together with a cross validation and used the validation split inside CV to early stop, which is not ideal since the split should be used only for evalaution. This way the model can "see" the data before evaluation, hence I did not use early stopping with CV at all and fixed the number of epochs to 3, as authors of BERT \cite{devlin2019bert} suggest. 
 All other hyperaparemeters remained unchanged. AdamW optimizer is used with the initial learning rate is 5e-5. 
 
 Baseline evaluation of all used Czech models can be seen in table \ref{table:3}. Final F1 score is averaged across all folds. For further experiments and tuning, I chose the two best performing models \textbf{RobeCzech} and \textbf{FERNET-C5}.
 
 
 
 
 \section{Hyperparameter tuning}
For choosing the best hyperparameters, I restricted the search space only to the combination of:
 \begin{itemize}
     \item \textbf{Batch size} $\in \{16,32\}$
     \item \textbf{Learning rate} $\in $ \{2e-5,3e-5,5e-5\}
     \item \textbf{Epochs} $\in \{2,3,4\}$
 \end{itemize}
 
 As the authors of the original BERT paper suggest. The models were very sensitive to hyperparameters. After tuning, overall best parameters were learning rate = 3e-5, batch\_size = 16 and number of epochs = 3  
 
  \input{my_modules/tables/baseline}

 
 
 \section{Combining Datasets}
 Here i can combine datasets. Trying all combinations would mean training 128 models.
 \subsection{Pretrianing all}
 I pretrained all to see which are generally best. Trained all and evaluated on babe, to see what is the most relevant. On the same valid splits.
 Pretrained odchylka is too little, it has no impact at all. Wiki works very good. -> oversampling? pretraining? Low quality of media bias data really doesnt work out. Too small to learn bias embeddings.
 Finetuning on small datasets is very unstable ~ weight inicializations are bad.
 \subsection{Pretraining on WNC}
 \subsection{Subjectivity pretraining + augmenting}
 Despite SUBJ task being subset of complex media bias, modelwas not able to learn both tasks at once. Hence used only for pretraining
 \subsection{Media Bias Pretraining + augmenting }
 had to be downsampled to 2.5 k
 \subsection{Combining all datasets together Without WNC}
 \subsection{only high quality}

 
 
 
 
\section{Inference on Czech News Samples}
\begin{itemize}
    \item Analysis and statistics
    \item Few words Article level
\end{itemize}

%\section{LIME analysis and demo}

\section{Multi-Task learning approach}\label{mtl}