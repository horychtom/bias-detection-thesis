{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dedde350-9a61-4121-98d5-4488a9d71c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making imports convenient\n",
    "import sys\n",
    "import os\n",
    "PATH=os.getcwd().split('/notebooks')[0]\n",
    "sys.path.insert(1, PATH)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset,concatenate_datasets\n",
    "import transformers\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding,AutoModelForSequenceClassification,AdamW,get_scheduler,TrainingArguments,Trainer,EarlyStoppingCallback\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from src.utils.myutils import *\n",
    "import yaml\n",
    "import json\n",
    "import logging\n",
    "logging.disable(logging.ERROR)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "model_name = 'ufal/robeczech-base'\n",
    "CONFIG_PATH = PATH + '/src/utils/config.yaml'\n",
    "SUBJ_MODEL_PATH = '/home/horyctom/bias-detection-thesis/src/models/trained/subj2_pretrained.pth'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "            output_dir = './',\n",
    "            num_train_epochs=3,\n",
    "            save_total_limit=2,\n",
    "            disable_tqdm=False,\n",
    "            per_device_train_batch_size=16,  \n",
    "            warmup_steps=0,\n",
    "            weight_decay=0.1,\n",
    "            logging_dir='./',\n",
    "            learning_rate=2e-5)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "transformers.utils.logging.set_verbosity_error()\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "441d71dc-f7dd-413b-8519-c40492495b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_eval(eval_preds):\n",
    "    metric = load_metric(\"f1\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3512cab8-31f4-423e-96d4-32f3de05c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_babe = load_dataset('csv',data_files = PATH + '/data/CS/processed/BABE/train.csv')['train']\n",
    "data_cwnc = load_dataset('csv',data_files = PATH + '/data/CS/processed/CWNC/train.csv')['train']\n",
    "\n",
    "with open(CONFIG_PATH) as f:\n",
    "    config_data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "model_name = 'ufal/robeczech-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False,padding=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "babe_tok = preprocess_data(data_babe,tokenizer,'sentence')\n",
    "cwnc_tok = preprocess_data(data_cwnc,tokenizer,'sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb6a1be-5e52-45ef-8d6b-72cfbc9f76f5",
   "metadata": {},
   "source": [
    "# SUBJECTIVE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6a9de8f-44d2-42a6-8642-3b6fe5b3d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = load_dataset('csv',data_files=PATH + '/data/CS/raw/SUBJ/subj.csv')['train']\n",
    "mpqa = load_dataset('csv',data_files=PATH + '/data/CS/raw/MPQA/mpqa.csv')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fbbbb51-1444-41a5-946f-0b7030e55ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7192dc6f4cae497fae2904715d6e5c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = concatenate_datasets([subj,mpqa]).shuffle(seed=42)\n",
    "subj_tok = preprocess_data(train,tokenizer,'sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78506a88-bd0e-4cbd-b684-c1f577ebadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_tok = subj_tok.train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa78184-ab7f-4f92-9416-7cb9b90b9daa",
   "metadata": {},
   "source": [
    "## Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "543c9043-00c1-4453-80dd-27566f3f61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_pretrain = TrainingArguments(\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32,  \n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    disable_tqdm = False,\n",
    "    warmup_steps=0,\n",
    "    save_total_limit=5,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'f1',\n",
    "    weight_decay=0.1,\n",
    "    output_dir = './',\n",
    "    learning_rate=4e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "727338af-72aa-4c96-8905-2585da130f5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='6450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 600/6450 03:12 < 31:19, 3.11 it/s, Epoch 0/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.483300</td>\n",
       "      <td>0.354484</td>\n",
       "      <td>0.853266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.371200</td>\n",
       "      <td>0.357872</td>\n",
       "      <td>0.877238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.349600</td>\n",
       "      <td>0.340647</td>\n",
       "      <td>0.874462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.330200</td>\n",
       "      <td>0.300629</td>\n",
       "      <td>0.895199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.324700</td>\n",
       "      <td>0.283653</td>\n",
       "      <td>0.893452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.302500</td>\n",
       "      <td>0.300223</td>\n",
       "      <td>0.890129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=2);\n",
    "model.to(device)\n",
    "trainer = Trainer(model,training_args_pretrain,train_dataset=subj_tok['train'],data_collator=data_collator,tokenizer=tokenizer,eval_dataset=subj_tok['test'],\n",
    "                          compute_metrics=compute_metrics_eval,callbacks = [EarlyStoppingCallback(early_stopping_patience=2)])\n",
    "trainer.train()\n",
    "torch.save(model.state_dict(),SUBJ_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def9547f-1f4a-47f3-b625-3decb308ca75",
   "metadata": {},
   "source": [
    "## Eval on BABE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "018d6c45-8418-42d0-ab34-b677f894b483",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py:44: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:135.)\n",
      "  return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 00:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 00:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 00:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 00:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 00:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = []\n",
    "for train_index, val_index in skfold.split(babe_tok['input_ids'],babe_tok['label']):\n",
    "\n",
    "    token_train = Dataset.from_dict(babe_tok[train_index])\n",
    "    token_valid = Dataset.from_dict(babe_tok[val_index])\n",
    "\n",
    "    torch.cuda.manual_seed(12345)\n",
    "    torch.manual_seed(12345)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=2);\n",
    "    model.load_state_dict(torch.load(SUBJ_MODEL_PATH))\n",
    "    #model.classifier.apply(model._init_weights)\n",
    "\n",
    "    model.to(device)\n",
    "    trainer = Trainer(model,training_args,train_dataset=token_train,data_collator=data_collator,tokenizer=tokenizer)\n",
    "    trainer.train()\n",
    "\n",
    "    #evaluation\n",
    "    eval_dataloader = DataLoader(token_valid, batch_size=BATCH_SIZE, collate_fn=data_collator)\n",
    "    scores.append(compute_metrics(model,device,eval_dataloader)['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf36edd7-2f18-49cd-b0bc-b59a93c4a731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7760311260935445"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9227521-bda9-4deb-9b91-83a163130112",
   "metadata": {},
   "source": [
    "## Eval on BABE with frozen encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5db5fb6-dd1e-4cf1-8b6d-e1a0e0929d38",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 00:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 00:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 00:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 00:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 00:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = []\n",
    "for train_index, val_index in skfold.split(babe_tok['input_ids'],babe_tok['label']):\n",
    "\n",
    "    token_train = Dataset.from_dict(babe_tok[train_index])\n",
    "    token_valid = Dataset.from_dict(babe_tok[val_index])\n",
    "\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=2);\n",
    "    model.load_state_dict(torch.load(SUBJ_MODEL_PATH))\n",
    "    #model.classifier.apply(model._init_weights)\n",
    "    #for name, param in model.named_parameters():\n",
    "    #    if 'classifier' not in name: # classifier layer\n",
    "    #        param.requires_grad = False\n",
    "\n",
    "    model.to(device)\n",
    "    trainer = Trainer(model,training_args,train_dataset=token_train,data_collator=data_collator,tokenizer=tokenizer)\n",
    "    trainer.train()\n",
    "\n",
    "    #evaluation\n",
    "    eval_dataloader = DataLoader(token_valid, batch_size=BATCH_SIZE, collate_fn=data_collator)\n",
    "    scores.append(compute_metrics(model,device,eval_dataloader)['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2a57d8d4-c740-4c96-896c-6f707c078258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7812235897435897"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not frozen, not initialized\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d1b5ac5c-7489-48ea-beb6-dda39b40f12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7799430769230769"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classifier layer renitialized\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06539f3c-669a-45bf-8d19-3a5b65d90328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.680008717948718"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#frozen encoder\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5bc182-422f-4e1d-9270-3a991b946a8e",
   "metadata": {},
   "source": [
    "## Eval on CWNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c9b60f46-ea4d-45fa-b948-a969016d8944",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='735' max='735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [735/735 01:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.416900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='735' max='735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [735/735 01:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.418400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='735' max='735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [735/735 01:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.417900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='735' max='735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [735/735 01:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.417000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='735' max='735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [735/735 01:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.410700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7744897959183674, 0.7785714285714285, 0.7540816326530613, 0.753061224489796, 0.746938775510204]\n",
      "0.7614285714285713\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for train_index, val_index in skfold.split(cwnc_tok['input_ids'],cwnc_tok['label']):\n",
    "\n",
    "    token_train = Dataset.from_dict(cwnc_tok[train_index])\n",
    "    token_valid = Dataset.from_dict(cwnc_tok[val_index])\n",
    "\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=2);\n",
    "    model.load_state_dict(torch.load(SUBJ_MODEL_PATH))\n",
    "    model.to(device)\n",
    "    trainer = Trainer(model,training_args,train_dataset=token_train,data_collator=data_collator,tokenizer=tokenizer)\n",
    "    trainer.train()\n",
    "\n",
    "    #evaluation\n",
    "    eval_dataloader = DataLoader(token_valid, batch_size=BATCH_SIZE, collate_fn=data_collator)\n",
    "    scores.append(compute_metrics(model,device,eval_dataloader)['f1'])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5f2e6eb3-7b70-445b-8f10-90fe7e215611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7744897959183674, 0.7785714285714285, 0.7540816326530613, 0.753061224489796, 0.746938775510204]\n",
      "0.7614285714285713\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00534870-2582-484c-bac9-a87e2e68c0fa",
   "metadata": {},
   "source": [
    "## Train Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "192e38c7-4264-413f-a2dd-d97d2f7beb27",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "Loading cached shuffled indices for dataset at /home/horyctom/.cache/huggingface/datasets/csv/default-9874b5367257af41/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-a4fadf39168fd11b.arrow\n",
      "***** Running training *****\n",
      "  Num examples = 28296\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5307\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5307' max='5307' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5307/5307 11:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.328400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.291500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.246400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.241400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.178600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.185700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.164100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoint-500\n",
      "Configuration saved in ./checkpoint-500/config.json\n",
      "Model weights saved in ./checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./checkpoint-1000\n",
      "Configuration saved in ./checkpoint-1000/config.json\n",
      "Model weights saved in ./checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./checkpoint-1500\n",
      "Configuration saved in ./checkpoint-1500/config.json\n",
      "Model weights saved in ./checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-2000\n",
      "Configuration saved in ./checkpoint-2000/config.json\n",
      "Model weights saved in ./checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-2500\n",
      "Configuration saved in ./checkpoint-2500/config.json\n",
      "Model weights saved in ./checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-3000\n",
      "Configuration saved in ./checkpoint-3000/config.json\n",
      "Model weights saved in ./checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-3500\n",
      "Configuration saved in ./checkpoint-3500/config.json\n",
      "Model weights saved in ./checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-4000\n",
      "Configuration saved in ./checkpoint-4000/config.json\n",
      "Model weights saved in ./checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-4500\n",
      "Configuration saved in ./checkpoint-4500/config.json\n",
      "Model weights saved in ./checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-5000\n",
      "Configuration saved in ./checkpoint-5000/config.json\n",
      "Model weights saved in ./checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-4000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "Loading cached shuffled indices for dataset at /home/horyctom/.cache/huggingface/datasets/csv/default-9874b5367257af41/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-1b42d3b03d5e43fc.arrow\n",
      "loading configuration file https://huggingface.co/ufal/robeczech-base/resolve/main/config.json from cache at /home/horyctom/.cache/huggingface/transformers/967e55aeea0667ffcda38959128e06f755d387fa034ffb448cab0851f27c5104.ae62083e57028e6866dba352dfd4261396c2f0e8978f299e3a17c055c564de09\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51961\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ufal/robeczech-base/resolve/main/pytorch_model.bin from cache at /home/horyctom/.cache/huggingface/transformers/a08b09622106c5b39eba8d15008b404679d71abc2258f2f4c1ecb50492c19db3.9845f6e21d70dd41c9b0343f0fe86ace188d94128a6cf9689a5e01a85794a850\n",
      "Some weights of the model checkpoint at ufal/robeczech-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ufal/robeczech-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 28296\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5307\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5307' max='5307' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5307/5307 11:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.432500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.368400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.330800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.286600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.256100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.251300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.240900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.183500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.183100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.164700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoint-500\n",
      "Configuration saved in ./checkpoint-500/config.json\n",
      "Model weights saved in ./checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-1000\n",
      "Configuration saved in ./checkpoint-1000/config.json\n",
      "Model weights saved in ./checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-1500\n",
      "Configuration saved in ./checkpoint-1500/config.json\n",
      "Model weights saved in ./checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-2000\n",
      "Configuration saved in ./checkpoint-2000/config.json\n",
      "Model weights saved in ./checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-2500\n",
      "Configuration saved in ./checkpoint-2500/config.json\n",
      "Model weights saved in ./checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-3000\n",
      "Configuration saved in ./checkpoint-3000/config.json\n",
      "Model weights saved in ./checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-3500\n",
      "Configuration saved in ./checkpoint-3500/config.json\n",
      "Model weights saved in ./checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-4000\n",
      "Configuration saved in ./checkpoint-4000/config.json\n",
      "Model weights saved in ./checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-4500\n",
      "Configuration saved in ./checkpoint-4500/config.json\n",
      "Model weights saved in ./checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-5000\n",
      "Configuration saved in ./checkpoint-5000/config.json\n",
      "Model weights saved in ./checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-4000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "Loading cached shuffled indices for dataset at /home/horyctom/.cache/huggingface/datasets/csv/default-9874b5367257af41/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-708f9b16d3ba3350.arrow\n",
      "loading configuration file https://huggingface.co/ufal/robeczech-base/resolve/main/config.json from cache at /home/horyctom/.cache/huggingface/transformers/967e55aeea0667ffcda38959128e06f755d387fa034ffb448cab0851f27c5104.ae62083e57028e6866dba352dfd4261396c2f0e8978f299e3a17c055c564de09\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51961\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ufal/robeczech-base/resolve/main/pytorch_model.bin from cache at /home/horyctom/.cache/huggingface/transformers/a08b09622106c5b39eba8d15008b404679d71abc2258f2f4c1ecb50492c19db3.9845f6e21d70dd41c9b0343f0fe86ace188d94128a6cf9689a5e01a85794a850\n",
      "Some weights of the model checkpoint at ufal/robeczech-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ufal/robeczech-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 28297\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5307\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5307' max='5307' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5307/5307 10:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.356400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.334300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.294300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.251300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.250100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.182500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.171400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.177200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoint-500\n",
      "Configuration saved in ./checkpoint-500/config.json\n",
      "Model weights saved in ./checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-1000\n",
      "Configuration saved in ./checkpoint-1000/config.json\n",
      "Model weights saved in ./checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-1500\n",
      "Configuration saved in ./checkpoint-1500/config.json\n",
      "Model weights saved in ./checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-2000\n",
      "Configuration saved in ./checkpoint-2000/config.json\n",
      "Model weights saved in ./checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-2500\n",
      "Configuration saved in ./checkpoint-2500/config.json\n",
      "Model weights saved in ./checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-3000\n",
      "Configuration saved in ./checkpoint-3000/config.json\n",
      "Model weights saved in ./checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-3500\n",
      "Configuration saved in ./checkpoint-3500/config.json\n",
      "Model weights saved in ./checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-4000\n",
      "Configuration saved in ./checkpoint-4000/config.json\n",
      "Model weights saved in ./checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-4500\n",
      "Configuration saved in ./checkpoint-4500/config.json\n",
      "Model weights saved in ./checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-5000\n",
      "Configuration saved in ./checkpoint-5000/config.json\n",
      "Model weights saved in ./checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-4000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "loading configuration file https://huggingface.co/ufal/robeczech-base/resolve/main/config.json from cache at /home/horyctom/.cache/huggingface/transformers/967e55aeea0667ffcda38959128e06f755d387fa034ffb448cab0851f27c5104.ae62083e57028e6866dba352dfd4261396c2f0e8978f299e3a17c055c564de09\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51961\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ufal/robeczech-base/resolve/main/pytorch_model.bin from cache at /home/horyctom/.cache/huggingface/transformers/a08b09622106c5b39eba8d15008b404679d71abc2258f2f4c1ecb50492c19db3.9845f6e21d70dd41c9b0343f0fe86ace188d94128a6cf9689a5e01a85794a850\n",
      "Some weights of the model checkpoint at ufal/robeczech-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ufal/robeczech-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 28297\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5307\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5307' max='5307' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5307/5307 10:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.453000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.331900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.249200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.247800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.237100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.171300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.176800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoint-500\n",
      "Configuration saved in ./checkpoint-500/config.json\n",
      "Model weights saved in ./checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-1000\n",
      "Configuration saved in ./checkpoint-1000/config.json\n",
      "Model weights saved in ./checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-1500\n",
      "Configuration saved in ./checkpoint-1500/config.json\n",
      "Model weights saved in ./checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-2000\n",
      "Configuration saved in ./checkpoint-2000/config.json\n",
      "Model weights saved in ./checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-2500\n",
      "Configuration saved in ./checkpoint-2500/config.json\n",
      "Model weights saved in ./checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-3000\n",
      "Configuration saved in ./checkpoint-3000/config.json\n",
      "Model weights saved in ./checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-3500\n",
      "Configuration saved in ./checkpoint-3500/config.json\n",
      "Model weights saved in ./checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-4000\n",
      "Configuration saved in ./checkpoint-4000/config.json\n",
      "Model weights saved in ./checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-4500\n",
      "Configuration saved in ./checkpoint-4500/config.json\n",
      "Model weights saved in ./checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-5000\n",
      "Configuration saved in ./checkpoint-5000/config.json\n",
      "Model weights saved in ./checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-4000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "loading configuration file https://huggingface.co/ufal/robeczech-base/resolve/main/config.json from cache at /home/horyctom/.cache/huggingface/transformers/967e55aeea0667ffcda38959128e06f755d387fa034ffb448cab0851f27c5104.ae62083e57028e6866dba352dfd4261396c2f0e8978f299e3a17c055c564de09\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51961\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ufal/robeczech-base/resolve/main/pytorch_model.bin from cache at /home/horyctom/.cache/huggingface/transformers/a08b09622106c5b39eba8d15008b404679d71abc2258f2f4c1ecb50492c19db3.9845f6e21d70dd41c9b0343f0fe86ace188d94128a6cf9689a5e01a85794a850\n",
      "Some weights of the model checkpoint at ufal/robeczech-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ufal/robeczech-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 28297\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5307\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5307' max='5307' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5307/5307 10:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.450500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.352400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.332400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.255600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.243800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.236400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.179100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.171700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.172500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoint-500\n",
      "Configuration saved in ./checkpoint-500/config.json\n",
      "Model weights saved in ./checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-1000\n",
      "Configuration saved in ./checkpoint-1000/config.json\n",
      "Model weights saved in ./checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-1500\n",
      "Configuration saved in ./checkpoint-1500/config.json\n",
      "Model weights saved in ./checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-2000\n",
      "Configuration saved in ./checkpoint-2000/config.json\n",
      "Model weights saved in ./checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-2500\n",
      "Configuration saved in ./checkpoint-2500/config.json\n",
      "Model weights saved in ./checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-3000\n",
      "Configuration saved in ./checkpoint-3000/config.json\n",
      "Model weights saved in ./checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-3500\n",
      "Configuration saved in ./checkpoint-3500/config.json\n",
      "Model weights saved in ./checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-4000\n",
      "Configuration saved in ./checkpoint-4000/config.json\n",
      "Model weights saved in ./checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-4500\n",
      "Configuration saved in ./checkpoint-4500/config.json\n",
      "Model weights saved in ./checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./checkpoint-5000\n",
      "Configuration saved in ./checkpoint-5000/config.json\n",
      "Model weights saved in ./checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-4000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7456, 0.744, 0.7147435897435899, 0.7259615384615384, 0.7211538461538461]\n",
      "0.7302917948717949\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for train_index, val_index in skfold.split(babe_tok['input_ids'],babe_tok['label']):\n",
    "\n",
    "    token_train = Dataset.from_dict(babe_tok[train_index])\n",
    "    token_valid = Dataset.from_dict(babe_tok[val_index])\n",
    "    \n",
    "    token_train = concatenate_datasets([token_train,subj_tok]).shuffle(seed=42)\n",
    "    \n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=2);\n",
    "    model.to(device)\n",
    "    trainer = Trainer(model,training_args,train_dataset=token_train,data_collator=data_collator,tokenizer=tokenizer)\n",
    "    trainer.train()\n",
    "\n",
    "    #evaluation\n",
    "    eval_dataloader = DataLoader(token_valid, batch_size=BATCH_SIZE, collate_fn=data_collator)\n",
    "    scores.append(compute_metrics(model,device,eval_dataloader)['f1'])\n",
    "        \n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ababf020-98a0-4e6c-8004-492b7a473527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7456, 0.744, 0.7147435897435899, 0.7259615384615384, 0.7211538461538461]\n",
      "0.7302917948717949\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
