{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making imports convenient\n",
    "import sys\n",
    "import os\n",
    "PATH=os.getcwd().split('/notebooks')[0]\n",
    "sys.path.insert(1, PATH)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset,concatenate_datasets\n",
    "import transformers\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding,AutoModelForSequenceClassification,AdamW,get_scheduler,TrainingArguments,Trainer,EarlyStoppingCallback\n",
    "\n",
    "from src.utils.myutils import *\n",
    "import yaml\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "CS_DATA_PATH = PATH + '/data/CS/processed/BABE/train.csv'\n",
    "CONFIG_PATH = PATH + '/src/utils/config.yaml'\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "transformers.utils.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BABE train_test split (SKIP IF DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-41acc90be2294f89\n",
      "Reusing dataset csv (/home/horyctom/.cache/huggingface/datasets/csv/default-41acc90be2294f89/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n"
     ]
    }
   ],
   "source": [
    "babe = load_dataset(\"csv\", data_files=PATH + '/data/CS/raw/BABE/SG2.csv')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/horyctom/.cache/huggingface/datasets/csv/default-41acc90be2294f89/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-2e89d302da86ff73.arrow and /home/horyctom/.cache/huggingface/datasets/csv/default-41acc90be2294f89/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-7a7ea8491428011d.arrow\n"
     ]
    }
   ],
   "source": [
    "babe = babe.train_test_split(0.15,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125614"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "babe['train'].to_csv(PATH + '/data/CS/processed/BABE/train.csv',index=False)\n",
    "babe['test'].to_csv(PATH + '/data/CS/processed/BABE/test.csv',index=False) #THIS IS FOR THE FINAL MODEL SELECTED,TUNED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f28f8af5b44ab214\n",
      "Reusing dataset csv (/home/horyctom/.cache/huggingface/datasets/csv/default-f28f8af5b44ab214/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'label'],\n",
       "    num_rows: 3122\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset('csv',data_files = CS_DATA_PATH)['train']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONFIG_PATH) as f:\n",
    "    config_data = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_eval(eval_preds):\n",
    "    metric = load_metric(\"f1\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=BATCH_SIZE,  \n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    eval_steps=20,\n",
    "    logging_steps=20,\n",
    "    disable_tqdm = False,\n",
    "    warmup_steps=0,\n",
    "    save_total_limit=5,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'f1',\n",
    "    weight_decay=0.1,\n",
    "    output_dir = './',\n",
    "    learning_rate=4e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Val all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/horyctom/.cache/huggingface/datasets/csv/default-f28f8af5b44ab214/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-74b152a0ca0f6626.arrow\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py:44: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:135.)\n",
      "  return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5-fold CV on model:  ufal/robeczech-base ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2372\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 380\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/380 : < :, Epoch 0.03/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_scores = {}\n",
    "\n",
    "for model_name in config_data['models']:\n",
    "    model_name='ufal/robeczech-base'\n",
    "    scores = []\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False,padding=True)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    token_full = preprocess_data(data,tokenizer,'sentence')\n",
    "\n",
    "    print(\"Running 5-fold CV on model: \",model_name,\"...\")\n",
    "    for train_index, val_index in skfold.split(token_full['input_ids'],token_full['label']):\n",
    "\n",
    "        token_train = Dataset.from_dict(token_full[train_index])\n",
    "        token_valid = Dataset.from_dict(token_full[val_index])\n",
    "\n",
    "\n",
    "        token_train = token_train.train_test_split(0.05)\n",
    "        train = token_train['train']\n",
    "        val = token_train['test']\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=2);\n",
    "        model.to(device)\n",
    "        trainer = Trainer(model,training_args,train_dataset=train,data_collator=data_collator,tokenizer=tokenizer,eval_dataset=val,\n",
    "                          compute_metrics=compute_metrics_eval,callbacks = [EarlyStoppingCallback(early_stopping_patience=3)])\n",
    "        trainer.train()\n",
    "\n",
    "        #evaluation\n",
    "        eval_dataloader = DataLoader(token_valid, batch_size=BATCH_SIZE, collate_fn=data_collator)\n",
    "        scores.append(compute_metrics(model,device,eval_dataloader)['f1'])\n",
    "        \n",
    "    print(\"Done.\")\n",
    "    model_scores[model_name] = scores\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UWB-AIR/Czert-B-base-cased F1 score: 0.7408641025641025\n",
      "ufal/robeczech-base F1 score: 0.7751410256410256\n",
      "bert-base-multilingual-cased F1 score: 0.7389446153846153\n",
      "fav-kky/FERNET-C5 F1 score: 0.7620046153846154\n",
      "fav-kky/FERNET-News F1 score: 0.7181794871794871\n",
      "DeepPavlov/bert-base-bg-cs-pl-ru-cased F1 score: 0.7604071794871794\n"
     ]
    }
   ],
   "source": [
    "for model in config_data['models']:\n",
    "    print(model,\"F1 score:\",np.mean(model_scores[model]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = token_full.train_test_split(0.1)\n",
    "token_train = token['train']\n",
    "token_val = token['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/ufal/robeczech-base/resolve/main/config.json from cache at /home/horyctom/.cache/huggingface/transformers/967e55aeea0667ffcda38959128e06f755d387fa034ffb448cab0851f27c5104.ae62083e57028e6866dba352dfd4261396c2f0e8978f299e3a17c055c564de09\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51961\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ufal/robeczech-base/resolve/main/pytorch_model.bin from cache at /home/horyctom/.cache/huggingface/transformers/a08b09622106c5b39eba8d15008b404679d71abc2258f2f4c1ecb50492c19db3.9845f6e21d70dd41c9b0343f0fe86ace188d94128a6cf9689a5e01a85794a850\n",
      "Some weights of the model checkpoint at ufal/robeczech-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ufal/robeczech-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 2809\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 440\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/440 04:16 < 00:25, 1.55 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.679100</td>\n",
       "      <td>0.621990</td>\n",
       "      <td>0.688963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.588200</td>\n",
       "      <td>0.540674</td>\n",
       "      <td>0.712329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.523400</td>\n",
       "      <td>0.558336</td>\n",
       "      <td>0.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.469300</td>\n",
       "      <td>0.531114</td>\n",
       "      <td>0.669291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.389400</td>\n",
       "      <td>0.534705</td>\n",
       "      <td>0.729323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.362900</td>\n",
       "      <td>0.578966</td>\n",
       "      <td>0.713805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.295200</td>\n",
       "      <td>0.695441</td>\n",
       "      <td>0.663900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.638551</td>\n",
       "      <td>0.696629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.228800</td>\n",
       "      <td>0.662417</td>\n",
       "      <td>0.738562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.142400</td>\n",
       "      <td>0.714533</td>\n",
       "      <td>0.725275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.156300</td>\n",
       "      <td>0.765672</td>\n",
       "      <td>0.748299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.814301</td>\n",
       "      <td>0.712727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.813674</td>\n",
       "      <td>0.743243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.098600</td>\n",
       "      <td>0.840888</td>\n",
       "      <td>0.750853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.079700</td>\n",
       "      <td>0.761885</td>\n",
       "      <td>0.721805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>1.007922</td>\n",
       "      <td>0.754967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>0.942294</td>\n",
       "      <td>0.751678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.040300</td>\n",
       "      <td>0.932641</td>\n",
       "      <td>0.744681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>0.969136</td>\n",
       "      <td>0.743682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.967253</td>\n",
       "      <td>0.735714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20/config.json\n",
      "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40/config.json\n",
      "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60/config.json\n",
      "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80/config.json\n",
      "Model weights saved in ./checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-100\n",
      "Configuration saved in ./checkpoint-100/config.json\n",
      "Model weights saved in ./checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-100/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-120\n",
      "Configuration saved in ./checkpoint-120/config.json\n",
      "Model weights saved in ./checkpoint-120/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-120/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-120/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-140\n",
      "Configuration saved in ./checkpoint-140/config.json\n",
      "Model weights saved in ./checkpoint-140/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-140/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-140/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-160\n",
      "Configuration saved in ./checkpoint-160/config.json\n",
      "Model weights saved in ./checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-180\n",
      "Configuration saved in ./checkpoint-180/config.json\n",
      "Model weights saved in ./checkpoint-180/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-180/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-180/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-200\n",
      "Configuration saved in ./checkpoint-200/config.json\n",
      "Model weights saved in ./checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-220\n",
      "Configuration saved in ./checkpoint-220/config.json\n",
      "Model weights saved in ./checkpoint-220/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-220/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-220/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-120] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-240\n",
      "Configuration saved in ./checkpoint-240/config.json\n",
      "Model weights saved in ./checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-140] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-260\n",
      "Configuration saved in ./checkpoint-260/config.json\n",
      "Model weights saved in ./checkpoint-260/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-260/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-260/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-280\n",
      "Configuration saved in ./checkpoint-280/config.json\n",
      "Model weights saved in ./checkpoint-280/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-280/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-280/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-180] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-300\n",
      "Configuration saved in ./checkpoint-300/config.json\n",
      "Model weights saved in ./checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-320\n",
      "Configuration saved in ./checkpoint-320/config.json\n",
      "Model weights saved in ./checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-220] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-340\n",
      "Configuration saved in ./checkpoint-340/config.json\n",
      "Model weights saved in ./checkpoint-340/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-340/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-340/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-360\n",
      "Configuration saved in ./checkpoint-360/config.json\n",
      "Model weights saved in ./checkpoint-360/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-360/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-360/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-260] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-380\n",
      "Configuration saved in ./checkpoint-380/config.json\n",
      "Model weights saved in ./checkpoint-380/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-380/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-380/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-280] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 313\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-400\n",
      "Configuration saved in ./checkpoint-400/config.json\n",
      "Model weights saved in ./checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-300] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./checkpoint-320 (score: 0.7549668874172185).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=0.23554600685834884, metrics={'train_runtime': 256.676, 'train_samples_per_second': 109.438, 'train_steps_per_second': 1.714, 'total_flos': 1184856915605040.0, 'train_loss': 0.23554600685834884, 'epoch': 9.09})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=2);\n",
    "model.to(device)\n",
    "trainer = Trainer(model,training_args,train_dataset=token_train,data_collator=data_collator,tokenizer=tokenizer,eval_dataset=token_val,\n",
    "                          compute_metrics=compute_metrics_eval,callbacks = [EarlyStoppingCallback(early_stopping_patience=4)])\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f9cc7747e44466b557949440cd069a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_full = preprocess_data(babe['test'],tokenizer,'sentence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8003629764065335"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataloader = DataLoader(token_full, batch_size=BATCH_SIZE, collate_fn=data_collator)\n",
    "compute_metrics(model,device,eval_dataloader)['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
