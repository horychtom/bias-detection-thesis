{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4db831f2-fa21-428c-87fe-983f42a5269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_metric,load_dataset,Dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding,RobertaForSequenceClassification,AdamW,get_scheduler,TrainingArguments,Trainer\n",
    "from corpy.morphodita import Tokenizer\n",
    "from newspaper import Article\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import csv\n",
    "import gc\n",
    "import re\n",
    "\n",
    "model_checkpoint = 'ufal/robeczech-base'\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "transformers.logging.set_verbosity(transformers.logging.ERROR)\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee713e6d-8dce-49b4-ba44-3b5fdd62579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "def compute_metrics(testing_dataloader):\n",
    "    metric = load_metric(\"f1\")\n",
    "    metric2 = load_metric(\"accuracy\")\n",
    "\n",
    "    model.eval()\n",
    "    for batch in testing_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        metric2.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "        \n",
    "    return (metric.compute(average='micro'),metric2.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d3d363-87e8-4793-b753-2b516be15bde",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f8bb581-daf7-4a7b-9307-fd2ca1ec7448",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/BABE/final_labels_SG2.csv',sep=';')\n",
    "data = data[['text','label_bias']]\n",
    "final_indices = data.index[data['label_bias'] != 'No agreement'].tolist()\n",
    "data = data[data['label_bias']!='No agreement']\n",
    "\n",
    "mapping = {'Non-biased':0, 'Biased':1}\n",
    "data.replace({'label_bias':mapping},inplace=True)\n",
    "data_en = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1900c9fd-e621-4de4-bd6c-8d7cf0544f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/BABE/texts_CS.txt','r') as f:\n",
    "    sentences = [sentence.strip('\\n') for sentence in f.readlines()]\n",
    "    sentences = list(filter(lambda x: len(x) != 0, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02a1fe92-b1d9-4659-8f5d-965610366066",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.array(sentences)[final_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a93b05a0-3df7-41f0-accd-dd366699749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_dict({'sentence':sentences,'label':data['label_bias']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23619da5-0547-460a-a81e-c79431289cc4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21e99242-8bec-4015-bad1-5b925931277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a55195a5-5837-4dff-a84d-dfe2afcfc99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False) #fast tokenizer is buggy in RoBERTa models\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64cb3511-6300-46bf-af7c-5be598f3813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = lambda data : tokenizer(data['sentence'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d815a4bb-ae2f-4ab7-ac31-20d63790fb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dab9c74f205449babf608713ecb296f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = data.map(tokenize,batched=True)\n",
    "tokenized_data = tokenized_data.remove_columns(['sentence'])\n",
    "tokenized_data.set_format(\"torch\")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7e2fd6d-7277-4487-a172-2f4f753365fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='../',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=BATCH_SIZE,  \n",
    "    logging_steps=25,\n",
    "    disable_tqdm = False,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b75f54-b906-44cb-9c52-ff713ff1d9bf",
   "metadata": {},
   "source": [
    "### 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "11d5d4e1-47e7-4431-89a9-ed37645b51d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d398fe99-daa1-49fa-acca-ecf7aa3f24db",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py:44: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:135.)\n",
      "  return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "***** Running training *****\n",
      "  Num examples = 2938\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 02:47, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.619100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.551800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.551100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.515900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.445800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.432100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.398400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.352800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.301300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.309500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.177200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.160700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.198700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.191300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.093100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.142300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.084300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.059100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.081700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.056300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.039300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.035900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.011500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../checkpoint-500\n",
      "Configuration saved in ../checkpoint-500/config.json\n",
      "Model weights saved in ../checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ../checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ../checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "loading configuration file https://huggingface.co/ufal/robeczech-base/resolve/main/config.json from cache at /home/horyctom/.cache/huggingface/transformers/967e55aeea0667ffcda38959128e06f755d387fa034ffb448cab0851f27c5104.ae62083e57028e6866dba352dfd4261396c2f0e8978f299e3a17c055c564de09\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51961\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ufal/robeczech-base/resolve/main/pytorch_model.bin from cache at /home/horyctom/.cache/huggingface/transformers/a08b09622106c5b39eba8d15008b404679d71abc2258f2f4c1ecb50492c19db3.9845f6e21d70dd41c9b0343f0fe86ace188d94128a6cf9689a5e01a85794a850\n",
      "Some weights of the model checkpoint at ufal/robeczech-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ufal/robeczech-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 2938\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 02:41, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.647000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.559100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.552500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.546900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.451600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.450900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.424900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.381200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.299600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.295500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.283100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.208800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.185500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.237900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.095400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.138500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.143300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.090300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.104600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.086900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.057600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.039800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.031900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.040100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.052600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.015100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../checkpoint-500\n",
      "Configuration saved in ../checkpoint-500/config.json\n",
      "Model weights saved in ../checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ../checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ../checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "loading configuration file https://huggingface.co/ufal/robeczech-base/resolve/main/config.json from cache at /home/horyctom/.cache/huggingface/transformers/967e55aeea0667ffcda38959128e06f755d387fa034ffb448cab0851f27c5104.ae62083e57028e6866dba352dfd4261396c2f0e8978f299e3a17c055c564de09\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51961\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ufal/robeczech-base/resolve/main/pytorch_model.bin from cache at /home/horyctom/.cache/huggingface/transformers/a08b09622106c5b39eba8d15008b404679d71abc2258f2f4c1ecb50492c19db3.9845f6e21d70dd41c9b0343f0fe86ace188d94128a6cf9689a5e01a85794a850\n",
      "Some weights of the model checkpoint at ufal/robeczech-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ufal/robeczech-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 2938\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 02:54, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.642500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.575300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.482500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.418700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.467300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.424500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.329100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.298600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.233000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.154400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.123500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.069400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.045600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.071100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.049100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.041100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.032700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.041400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.038100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../checkpoint-500\n",
      "Configuration saved in ../checkpoint-500/config.json\n",
      "Model weights saved in ../checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ../checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ../checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "loading configuration file https://huggingface.co/ufal/robeczech-base/resolve/main/config.json from cache at /home/horyctom/.cache/huggingface/transformers/967e55aeea0667ffcda38959128e06f755d387fa034ffb448cab0851f27c5104.ae62083e57028e6866dba352dfd4261396c2f0e8978f299e3a17c055c564de09\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51961\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ufal/robeczech-base/resolve/main/pytorch_model.bin from cache at /home/horyctom/.cache/huggingface/transformers/a08b09622106c5b39eba8d15008b404679d71abc2258f2f4c1ecb50492c19db3.9845f6e21d70dd41c9b0343f0fe86ace188d94128a6cf9689a5e01a85794a850\n",
      "Some weights of the model checkpoint at ufal/robeczech-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ufal/robeczech-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 2939\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 02:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.645500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.552600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.500800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.495600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.479200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.401200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.402200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.362400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.295900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.305200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.197200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.182900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.213600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.217500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.098700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.139200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.085700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.066800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.066600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.036700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.032800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../checkpoint-500\n",
      "Configuration saved in ../checkpoint-500/config.json\n",
      "Model weights saved in ../checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ../checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ../checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "loading configuration file https://huggingface.co/ufal/robeczech-base/resolve/main/config.json from cache at /home/horyctom/.cache/huggingface/transformers/967e55aeea0667ffcda38959128e06f755d387fa034ffb448cab0851f27c5104.ae62083e57028e6866dba352dfd4261396c2f0e8978f299e3a17c055c564de09\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51961\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ufal/robeczech-base/resolve/main/pytorch_model.bin from cache at /home/horyctom/.cache/huggingface/transformers/a08b09622106c5b39eba8d15008b404679d71abc2258f2f4c1ecb50492c19db3.9845f6e21d70dd41c9b0343f0fe86ace188d94128a6cf9689a5e01a85794a850\n",
      "Some weights of the model checkpoint at ufal/robeczech-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ufal/robeczech-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 2939\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 02:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.643000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.538500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.496700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.447900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.386100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.423300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.311100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.329500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.282800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.161900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.175400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.111800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.057100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.069400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.061400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.014600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../checkpoint-500\n",
      "Configuration saved in ../checkpoint-500/config.json\n",
      "Model weights saved in ../checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ../checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ../checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for train_index, val_index in skfold.split(tokenized_data['input_ids'],tokenized_data['label']):\n",
    "    \n",
    "    token_train = Dataset.from_dict(tokenized_data[train_index])\n",
    "    token_valid = Dataset.from_dict(tokenized_data[val_index])\n",
    "    \n",
    "    model = RobertaForSequenceClassification.from_pretrained(model_checkpoint);\n",
    "    trainer = Trainer(model,training_args,train_dataset=token_train,data_collator=data_collator,\n",
    "                      tokenizer=tokenizer)\n",
    "    trainer.train()\n",
    "    \n",
    "    #evaluation\n",
    "    eval_dataloader = DataLoader(token_valid, batch_size=BATCH_SIZE, collate_fn=data_collator)\n",
    "    scores.append(compute_metrics(eval_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "220ab629-9d51-46ee-b8f7-a96dcc5c9226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(),'../cs_babe.pth')\n",
    "model.load_state_dict(torch.load('../cs_babe.pth'))\n",
    "model.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b73aea12-d63a-4db8-b051-8c174d450e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'f1': 0.782312925170068}, {'accuracy': 0.782312925170068}),\n",
       " ({'f1': 0.7945578231292517}, {'accuracy': 0.7945578231292517}),\n",
       " ({'f1': 0.7904761904761904}, {'accuracy': 0.7904761904761904}),\n",
       " ({'f1': 0.7656675749318801}, {'accuracy': 0.7656675749318801}),\n",
       " ({'f1': 0.7697547683923706}, {'accuracy': 0.7697547683923706})]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc734a9-1926-4a92-b89d-f510f41b8d1d",
   "metadata": {},
   "source": [
    "## Training on full data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b5279-f1c0-4900-bdbf-21869293b16a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3673\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1150\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='766' max='1150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 766/1150 02:27 < 01:13, 5.20 it/s, Epoch 6.65/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.640600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.546600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.556500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.479700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.445900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.405400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.412300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.441100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.331100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.269400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.236500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.332100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.226100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.204900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.163700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.114300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.153400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.130100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.081100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.086200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.076300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.076200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.078200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../checkpoint-500\n",
      "Configuration saved in ../checkpoint-500/config.json\n",
      "Model weights saved in ../checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ../checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ../checkpoint-500/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(model_checkpoint);\n",
    "trainer = Trainer(model,training_args,train_dataset=tokenized_data,data_collator=data_collator,\n",
    "                      tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "005f7924-fc91-4822-8d7b-e1ab62139f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'../cs_babe.pth')\n",
    "model.load_state_dict(torch.load('../cs_babe.pth'))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43ec87-2cd2-4af8-bf0c-5d8527fdcef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b17a7d8-b8e7-45fc-aa8e-011d5c805e72",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inferrence experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea92e7f9-a0c0-41e1-bfcd-4398bcff1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence(sent:str):\n",
    "    toksentence = tokenizer(sent,truncation=True,return_tensors=\"pt\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        toksentence.to(device)\n",
    "        output = model(**toksentence)\n",
    "    \n",
    "    classification = F.softmax(output.logits,dim=1).argmax(dim=1)\n",
    "    \n",
    "    return {0:'unbiased',1:'biased'}[classification[0].item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f50d87-9d93-4cab-94df-2c873f7b26ce",
   "metadata": {},
   "source": [
    "### Nice pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c58106b9-13ca-4e0b-a60b-7cc81f313898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unbiased\n",
      "biased\n"
     ]
    }
   ],
   "source": [
    "print(classify_sentence(\"Od posledního Štědrýho večera a následující noci na mě máma ani jednou nepromluvila.\"))\n",
    "print(classify_sentence(\"Od posledního Štědrýho večera a následující noci na mě guru máma ani jednou nepromluvila.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d632fcdc-d255-442a-9b59-d5173c7b4fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unbiased\n",
      "biased\n"
     ]
    }
   ],
   "source": [
    "print(classify_sentence('Podle íránské vlády bylo sestřelení IR 655 \"Vincennes\" úmyslně provedeným a nezákonným činem.'))\n",
    "print(classify_sentence('Sestřelení IR 655 \"Vincennes\" bylo úmyslně provedeným a nezákonným činem.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f665100b-40e7-4a58-8e3d-30e4972e017b",
   "metadata": {},
   "source": [
    "### CW-hard biased data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d5fd9b8d-d323-46d2-8202-51e21ceaed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"biased_cwhard_cs.txt\",\"r\") as f:\n",
    "    cw_hard_cs = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0adf11f4-afcf-4bb9-9170-c078900ae453",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = np.array(list(zip(cw_hard_cs,list(map(classify_sentence,cw_hard_cs)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6cb719bd-3c93-40a9-bf71-de0bf9059ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias level:  32.44709712425393 %\n"
     ]
    }
   ],
   "source": [
    "stats = np.unique(annotations[:,1],return_counts=True)\n",
    "print(\"bias level: \",stats[1][0]/stats[1].sum()*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa33c1a4-b355-4f3a-894c-653ac3bf4fe8",
   "metadata": {},
   "source": [
    "### Try on any article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e4aadf2c-46d8-48e3-9ce2-cb7d280fe1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias level:  52.32974910394266 %\n"
     ]
    }
   ],
   "source": [
    "article = Article('https://nazory.aktualne.cz/komentare/jak-jsem-na-stedrej-den-zradila-nasi-antivax-familiji/r~93a757c460d211eca1070cc47ab5f122/')\n",
    "article.download()\n",
    "article.parse()\n",
    "\n",
    "tokenizer_morphodita = Tokenizer(\"czech\")\n",
    "\n",
    "all = []\n",
    "for sentence in tokenizer_morphodita.tokenize(article.text, sents=True):\n",
    "    all.append(sentence)\n",
    "    \n",
    "sentences = np.array([' '.join(x) for x in all])\n",
    "annotations = np.array(list(zip(sentences,list(map(classify_sentence,sentences)))))\n",
    "stats = np.unique(annotations[:,1],return_counts=True)\n",
    "\n",
    "print(\"bias level: \",stats[1][0]/stats[1].sum()*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "45aeee91-89f1-4b1f-93a1-476957131642",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Od posledního Štědrýho večera a následující noci na mě guru máma ani jednou nepromluvila .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['\" Zrádce národa , \" vykřikla máma a bouchla dlaní do volantu , \" zrádce zasranej \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Jo , abych nezapomněla , ono to sem patří , zrádcem národa je nějakej epidemiolog , jméno mi uteklo , kterej nabádá , ať se lidi očkujou .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['On je typickej podpantoflák , ale poslední dobou se taky maličko změnil a občas aspoň pípne , asi už je toho i na něj trochu moc .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Jemu neodpověděla , ale otočila se na mě dozadu , přitom jela na úzký silnici přes osmdesát , a povídá : \" Chápej , Mončo , \" ježíšmarjá , jak já nenávidím to svý jméno , Monika , ale k úplnýmu šílenství mě dohání Monča , Mončičák a Mončičáček , vraždila bych kvůli tomu , \" neříkej mi Mončo , prosím , \" zaúpěla jsem a ona sadisticky , \" chápej , Mončičáčku , \" dodávám , že mi je dvaadvacet , ne čtyři , \" já do tý zatáčky vi - dě - la . \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Hele , já samozřejmě věděla , že jsem s nima neměla jezdit , jasně že jo , věděla jsem , že to bude peklo , navíc tam přijede ségra , což je moje máma na druhou , takže peklo na druhou , jenomže se ještě nikdy nestalo , že bych s celou naší rodinou netrávila Štědrej den .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['No a mně je blbý jim to kazit , oni by mou nepřítomnost , jak řekla máma , když jsem se z toho letos snažila vyšmyknout , považovali za \" znamení , že s námi nechceš nic mít \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Hele , upřímně , týhle generaci fakt nic nevysvětlíš , je to marný .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Související Takový obyčejný covidový Štědrý večer sběhnuvší se v Podkrkonoší',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Mlčela jsem , protože to byl jedinej způsob , jak mámu donutit , aby aspoň chvíli koukala na silnici jiným než duchovním zrakem .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Táta teda dává pozor , když ona kouká dozadu , jenomže on je naprosto zoufalej šofér , takže kdyby sáhl na volant , to si pište , že by nás strhnul rovnou do protijedoucího autobusu .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Chápejte , tahle famílije miluje , když se množí , milujou , když jich je čím dál víc , nějaký přelidnění jim vůbec nic nepovídá , takže když jsem řekla , že s sebou vezmu svýho kluka , chodíme spolu už tři měsíce a jeho rodiče už od jeho osmnácti žijou v Jižní Americe , tak nadšeně kejvla .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Na to jsem zalhala , že nevím , i když samozřejmě vím , že je očkovanej , ale když už s nima jedu , chci ho mít s sebou , abych to přežila , a chci ho ušetřit jejích ujetejch ezoterickejch přednášek , opravdu nerada bych o něj přišla , dneska najít kluka je fakt porod .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Nejsem si s ním sice úplně \" sure \" , to ne , ale to neznamená , že budu další měsíce sama .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['No a tady jsme u toho , radši to vysypu hned , protože to je teď , jak říká máma , \" asi to úplně nejdůležitější \" , totiž očkování proti covidu a její boj proti \" tomu neskutečnému svinstvu \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Jenomže se jí to nepovedlo , nebo jim to nefungovalo , nebo co , nevím , pak to zkusila další den a další a nic , dneska si myslím , že něco musela dělat blbě , ona není zrovna ajťák , pořád se jí to nedařilo , tak jsem nabídla , že jí pomůžu , ale ona klasicky \" nejsem snad ještě úplnej blbec , nedělej ze mě tak strašně neschopnou ženskou , vy mladý si myslíte , že umíte všecko nejlíp \" , a zase se jí to nepovedlo .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Seděla u nás v kuchyni a křičela na svítící tablet \" já ten krám nenávidím \" a bušila pěstí do stolu , až mi vylila kafe .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Andělské kyvadlo , promluv !', 'biased'], dtype='<U588'),\n",
       " array(['Další den jsem ji viděla v kuchyni , jak sedí s tím svým milovaným andělským kyvadlem - ona už je totiž dávno \" ezo \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Soustředila se na ten řetízek s vybroušeným průhledným jehlanem na konci úplně propadlá do sebe , koukala , jak se kejvá , nebo nekejvá , nebo co , o mně nevěděla .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Řekla mi , že se \" jenom dotazuje \" .', 'biased'], dtype='<U588'),\n",
       " array(['Povídám , na co a čeho nebo koho se dotazuje , načež řekla nepřítomným hlasem , \" tím \" hlasem , rozuměj ezo - hlasem , takovým cizím , vzdáleným , ať ji laskavě neruším , tak jsem seděla a koukala na ni , dlouho to trvalo , a najednou vykřikla \" tak ne ! \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['A hned vytočila číslo ségry a hustila do ní , jak jí andělský kyvadlo řeklo , že ne , ale ne že ne jen ona , ale nikdo z nás ne , celá rodina ne , a že je to \" otázka života a smrti \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['S andělským kyvadlem blbne už dlouho , s tátou to vůbec nemáme rádi , ale jí se fakt těžko odporuje , ona je vůdčí , autoritativní typ .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Když jsem se jí , mimochodem ironicky , zeptala , jak to může fungovat , najednou mluvila o dost jiným hlasem , jako by ani nevycházel z ní , tak jako tiše a chraplavě až sípavě , a vysvětlila mi - jasně že nevysvětlila - jak se \" tázající musí úplně oprostit od svého ega , \" v týhle poloze hned přejde do spisovný mluvy , \" a hlavně , hlavně od veškeré racionality , poté musí začít sledovat jemné , takřka nepostřehnutelné , mimovolné svalové pohyby , jež vycházejí z nervové soustavy , jež v tu chvíli na sebe bere roli vodiče univerzální energie a umožní ony mimovolné pohyby svalů .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['\" E - ner - gi - i , \" řekla důrazně , protože jí jako zasvěcený je zřejmě jasný , co je to za energii , zatímco nezasvěcenejm mimoňům mýho typu to nemá smysl vykládat .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Dost mě zarazilo , jak najednou mluví o práci s kyvadlem .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Proboha , jaká práce ?', 'biased'], dtype='<U588'),\n",
       " array(['\" To by nemělo smysl , \" řekla mi tehdy příkře . - Souhlasím .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['No a od tý doby , co se nedokázala zaregistrovat na první dávku , se proměnila v totální antivaxerku .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Ségra to samý , jen ještě umocněnější , Hedvika je totiž navíc pekelně militantní a každej , kdo se očkuje , je podle ní idiot a zrádce národa - to má máma od ní - a \" otrok bez vlastní vůle \" a ubožák a \" člověk bez vlastního já \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Mně ségra řekla asi stokrát , že jsem \" jen otrok bez vlastní vůle \" , nikdy mě neměla ráda , nevím proč , a mám pocit , že covid jí k tomu dodal nějakou novou , máma promine , e - ner - gi - i .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Abyste si nemysleli , nejsem žádnej rodinnej odbojář , stejně jako táta není žádnej antimámovskej partyzán , to ne .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Takže zrazujeme jenom sami sebe .', 'biased'], dtype='<U588'),\n",
       " array(['Snad nemusím ani dodávat , že máma i ségra jsou samozřejmě taky proti rouškám a respirátorům , všude vykládaj , jak jsou nebezpečný , a obě choděj na antivaxerský demošky .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Takže jsem jí Péťu musela představit a ona na něj hned začala chrlit , v jak strašný době , v jak strašný nesvobodě , totalitě žijeme .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Viděla jsem , jak jí nerozumí , sypala to na něj jak štěrk z náklaďáku a on je takovej tichej , to se mi na něm líbí , nic nikomu necpe , a přitom je chytrej , ví úplně všechno , ale drží se stranou , a když jsme pak spolu , sami dva , tak říká neuvěřitelný věci , úplně se stydím , jak já nic nevím , kdežto on ví fakt všecko .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Máma s ním stála v předsíni a kázala mu , během pár měsíců se z ní stala fakt kazatelka , jak \" nám oni jen ničí imunitu , protože imunita , Přirozená Imunita \" , ona to opravdu vyslovuje jako Přirozená Imunita , s jakousi úctou a pokorou , vždycky si toho všimnu a vždycky mě to dost vyděsí , čím dál víc mě ta její Přirozená Imunita děsí , \" tedy to podstatné , to , oč tady jde , je naším úkolem zde na Zemi \" , jo , Zemi taky vyslovuje s velkým Z , \" máme pěstovat imunitu , otužovat se , cvičit , ale hlavně , jak se jmenuje , Mončo ? \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Stála jsem tam s nimi a modlila se , aby si Péťa nemyslel , že jsem stejnej magor jako máma , ale on , jak je fakt chytrej , tak prostě jen stál a mlčel a držel , neusmíval se , tvářil se smrtelně vážně a to se mámě líbilo .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Pak mi řekla , že \" to bude nějakej vnímavej kluk , toho se drž , ten by se mohl naučit i s andělským kyvadlem , když ty to jako agnostik odmítáš \" . - Agnostik ?',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Co blbne ?', 'biased'], dtype='<U588'),\n",
       " array(['Kouk se na mě a povídá \" nic \" .', 'biased'], dtype='<U588'),\n",
       " array(['\" Jak nic ?', 'biased'], dtype='<U588'),\n",
       " array(['To kázání v předsíni s tebou nic neudělalo ? \"', 'biased'],\n",
       "       dtype='<U588'),\n",
       " array(['No a teď sedí vedle mě v autě a nemůžu nevidět , jak má při tý šílený jízdě zaťatý všechny svaly , protože se bojí .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Po revoluci mu ho vrátili , máma statek udržuje , stará se o něj , cpe do něj neskutečný prachy a dala ho celý rodině k dispozici .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Abyste to správně chápali , máma je hodná , obětavá , štědrá , to jen ten covid jí uvrhnul do takový divný , kazatelský , nenávistný polohy .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Všichni už byli na statku nasáčkovaný a všichni samozřejmě nenaočkovaný a bez respirátorů , protože od léta máma všechny fakt dokonale zblbla .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Ona prostě je silná , přesvědčivá osobnost , notabene oni tři , oba strejdové a máma , to asi maj nějak geneticky daný , děda byl přesně tenhleten typ \" vždycky proti všemu a proti všem \" , samozřejmě proti komunistům , to po něm ale máminý bratři nepodědili , a zároveň by se rozdal , a všecko pro rodinu .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Já to měla horší , celá ta parta jsou antivaxeři a antirouškaři a vyznavači máminý Přirozený Imunity , což dávají najevo opravdu brutální líbačkou , dřív to zdaleka tolik neprovozovali .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Prostě se na člověka vrhnou , mám při tom neodbytnej pocit , že jim covid sedí přímo v mozkovým centru a nařizuje : teď na ni skoč a žádný na tvář , líbej ji na rty .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Dovedete si asi představit , jak je to nechutný .', 'biased'],\n",
       "       dtype='<U588'),\n",
       " array(['Jeden imunolog totiž někde na netu říkal , že funguje pravidelný vyplachování a kloktání , konkrétně zmínil vincentku do nosu a listerinem \" prokloktávat \" - to slovo je boží - krk .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Mužská část rodiny vyjma mýho táty a Péti , kterej vypadal po celou dobu pobytu dost vyděšeně a nebyla jsem si jistá , jestli se mnou zůstane i po tomhle anti Štědrým dnu , se intenzivně ethylizovala slivovicí od strejdy č . 2 , teda mladšího strejdy .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Manželka strejdy č . 1 , tedy staršího strejdy , poznamenala , že považuje za velmi rozumné dát nejdřív děti spát , a chce jen upozornit , že Honzík - tříletej , nesnesitelně rozmazlenej harant , kterej si všecko vynucuje intenzivním ječením a řevem a rodiče mu to tolerujou , protože \" dítě se musí nechat projevit a nesmí se omezovat \" - má prej \" už dva dny bolesti bříška a strašný , ale opravdu strašný průjem , dokonce chudáček i zvracel \" , takže k němu asi bude muset občas odběhnout a \" doufám , že vás Honzík nenakazí \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Máma řekla tím svým nově objeveným , kněžským tónem a jaksi za nás za všechny : \" Hanko , neboj se , my máme přece Přirozenou Imunitu . \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['No , nezdálo se mi , že by se všem díky tý její slavný P . I . nějak ulevilo , a zaslechla jsem , jak strejda č . 2 říká tetě č . 2 \" taky to mohla říct dřív \" , za což ho moje máma , který nic neujde , sjela velmi zlým pohledem , jaký vrhá guru se svatozáří na nevěřícího psa .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Při obědě mi bylo líto táty , on jako by tam snad ani nepatřil , jako by ho tam byla zapomněla nějaká jiná rodina .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Po obědě jsem šla za ním a povídám \" tati , dobrý ? \" a on se jen usmál a pokrčil rameny a velmi potichu řekl \" hele , já vím a ty víš , nepatřím sem \" , což je svatá pravda .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Usmál se tak vděčně , že mi ho bylo líto ještě mnohem víc .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Pak jsme šli s Péťou do mýho ajnclíku .', 'biased'], dtype='<U588'),\n",
       " array(['Když jsem na statku , tak tam mám vyhrazenou takovou nudli komůrku na kraji sešikmený střechy , nedá se tam sice stát , jen ležet , ale mám tam dlouhou a dost širokou postel , kam jsme si spolu vlezli a milovali se , což Péťovi jde , asi že u toho může bejt oficiálně zticha .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Kdysi máma pouštěla televizi , pohádky a koledy , ale postupně se to měnilo a poslední léta byla vždycky celorodinná kontemplace , při níž šlo především o to , aby rodiče dokázali zpacifikovat ty svý neskutečně ukňouraný děti .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['A loni , loni se máma pokoušela věštit budoucnost celý famílije pomocí andělskýho kyvadla .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Mimochodem , ne že bych tý nádobce ve tvaru jehlanu z vybroušenýho kamene zavěšený na stříbrným řetízku věřila , ale to , že pandemie covidu neskončí , a to , že nás čeká \" velká zatěžkávací zkouška \" , to teda kyvadlo trefilo celkem přesně .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Před večeří jsme si všichni včetně harantů museli povinně poslechnout toho herce Duška , co chodí bosej , kolem hlavy má auru jak svatej na mostě , prsí se šíleně nezlomnou imunitou a věří na sebe , nikoli na vědu a očkování .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Péťa to poslouchal , pak se na mě kouk , a já viděla , co mu šrotuje v tý jeho chytrý , mlčenlivý , fešácký hlavě , \" proboha , kam jsem to zapad \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Několik tejdnů po týhle naší zatěžkávací zkoušce mi zničehonic řekl \" já tam ten večer fakt myslel , že snad dojde i na nějakou dětskou oběť \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Vypla jsem ve chvíli , kdy Dušek řekl \" jsem takovej člověk , kterej od roku 1991 neměl v těle lék , já nechodím vlastně k doktorům , dělám půsty , plavu ve studený vodě , v řece , cvičím dechový cvičení \" , protože já chodím odmalička k doktorům a nebejt léků , už tu nejsem , ale to sem nepatří .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Pak se jedlo , a ta štědrovečerní tabule byla taky o hodně jiná než jindy , málem bych řekla , že covid všecko změnil , ale blbost , to my jsme všecko změnili .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Jenomže někdo už kapra nechce , tak se servíroval losos , filé , řízky a kuřecí stehna jak někde v restauraci s dlouhým jídelníčkem .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Pak šly tety dát spát děti a přijela Hedvika , moje ségra , s tím týpkem , co s ním žije , ale nevzali se .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['On je hodně zazobanej , má novýho bavoráka , \" BMW je auto \" je jeho oblíbená a hojně opakovaná hláška , jako že nic jinýho auto není , a následující hláška vždycky zní \" Tesla není auto \" , protože elektroauta tenhle mudrc považuje za \" největší omyl v technické historii lidstva \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Mává sice těmahle hesílkama , ale naše Hedvika ho řídí jak skútr , o tom žádná .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Takže je samozřejmě antivaxer a chodí s nima na demošky , přičemž jeho osobním přínosem je , že se vždycky zabalí do český vlajky .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Když tam s ním za mnou Hedvika nedávno přišla , Evžen si s tou vlajkou chvíli hrál - jo , on se fakt jmenuje Evžen , pekelný jméno - , a pak povídá \" blbej počet hvězd , Mončo , sorry jako \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Já jen abyste měli představu , co je Bavorák za týpka .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Ségra se na mýho kluka hned vrhla , skočila na něj jak tygr na antilopu , vypálila mu pusu na rty , a nekecám , fakt to vypadalo , jako kdyby se do něj zakousla , a ječela \" jóóó , tak ty jseš ten Péé - ťáá , jóóó ? \" přitom já jsem jí o něm nic neřekla , takže asi máma .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Péťa řekl \" ano , já jsem Petr \" a já se jen divila , že mu ze rtů nestříká červená , možná to jako medik umí nějak zastavit .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Strejda č . 2 všechny oběhl , on mámu odjakživa poslouchá na slovo , což tetu č . 2 poměrně dost nadzvedává , ale nikdy mámě nic neřekne , ať jdou dolů .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Tříletý bolavý bříško nějakým zázrakem usnulo , buď mu máma poslala kus tý svý svatý energie , nebo mu dali cucat cumel s mákem .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Podle vzoru mý mámy prakticky všichni vězeli v tom nejtěsnějším covidovým klinči a mumlali něco jako \" hlavně Sílu , Energii a Přirozenou Imunitu \" a dávali líbačku .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Péťa si držel distanc a hučel něco jako \" pěkný svátky \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Mám dojem , že se mu podařilo s nikým se neobjímat , což je , hádám , podobnej úspěch jako dát Ká dvojku bez kyslíkovejch přístrojů .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Po covidem pohrdajícím vánočním vinšování následovala společná meditace .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Chlapi odnesli stoly , sedělo se na zemi , komu klouby dovolily , seděl v lotosáku , mimochodem Péťa dá lotos úplně bez problémů , on je fakt mimořádnej , komu klouby nedovolily , tak se tam nějak nepohodlně , ale poslušně krčil .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Guru máma pustila uhozený meditativní bzučení - hučení - mručení , přivezla si na to nový velký bluetoothový džíbíelko , který se jí stopro pokusím během ledna nenápadně zabavit .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Meditace trvala přesně 27 minut a končila tak , že hudba šla do ztracena a naše kněžka začala hrozně divně hučet zavřenou pusou a nosem a postupně se přidávali všichni kromě Péti , mě a táty , ale táta tam vlastně nebyl .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Někam se vypařil .', 'biased'], dtype='<U588'),\n",
       " array(['Jak jsem tak koukala na příbuzný okolo sebe , zdáli se mi nějaký bledý a z formy , divný , ale ono se taky při meditaci svítilo jen svíčkama , tak to asi zkreslovalo .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Měla takový divný oči , jako by plovoucí , podobně vypadá jeden můj kolega z filozofický fakulty , kterej droguje , taky tak jako vždycky plave , a povídá mu : \" Dokázal jsi vnímat tu společnou Energii a Sílu ? \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Péťa se kouk na mě , pak na ni , pak do země a neřek nic .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Zdálo se mi , že Péťa poprvé ožil , a byla jsem v šoku , kolik je schopen toho sladkýho hnusu s pomerančovou šťávou do sebe vylejt , a taky že byl už zhruba za půl hodiny docela fest vylitej a obličej mu zčervenal tak , jak jsem to u něj za celý ty tři měsíce našeho chození neviděla .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Jak se vylil , měl se ke mně mnohem víc , což , musím říct , mi nebylo nepříjemný , protože v celý tý naší rodině se vlastně nikdo k nikomu nijak nemá .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Procházíme se po žhavým', 'biased'], dtype='<U588'),\n",
       " array(['Všichni jsme si museli stoupnout okolo toho tátova improvizovanýho žhaviště , některý už docela slušně vrávorali , Péťa v tom přímo exceloval , a máma držela štědrovečerní promluvu , jakousi modlitbu k Přirozený Imunitě nebo jak to nazvat .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Zakončila ji slovy \" a nyní vás zvu , vyzujte si střevíce \" , nikdy jsem ji neslyšela použít slovo střevíce , jako kdybychom se ocitli o sto let zpátky , \" a projděte se po žhavém uhlí na důkaz vaší osobní , neimplantované odolnosti \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Nejdřív jsme se ale museli společně rozdýchat , pak jsme točili rukama okolo hlavy a zase jsme měli hučet , stejně jako hučela máma zavřenou pusou a nosem .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Nikdo ani nepípl , ale můj táta , nikdy bych to něj neřekla , do ticha nahlas pravil , že \" samozřejmě nikdo není nucen \" , neboť \" my , oproti státu , nikoho k ničemu nenutíme a chůze po žhavém uhlí je zcela dobrovolná \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Viděla jsem , jak se mámě nelíbí , že si dovolil něco říct .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Pak se stalo ještě něco , co určitě nebylo předem dohodnutý , táta si totiž sám zul svoje černý , slavnostní polobotky , který nosí výhradně do divadla , stáhl si i svý šedý ponožky , přičemž bylo vidět , že má na obou palcích a na levý patě obrovský ďuzny , napadlo mě , že přesně proto se zul , že sleduju jeho jakousi tichou pomstu naší kněžce za všechnu tu měsíce či spíš roky trvající manipulaci , šel do čela k mámě , ohrnul si nohavice , a aniž by se jí zeptal , přičemž bylo jasný , že první chtěla šlapat po žhavým právě ona , vyrazil .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Nijak nespěchal , prostě to svý žhaviště přešel a mě napadlo , že to je celý možná jen nějakej fake , že to nejsou pravý žhavý uhlíky , dokonce jsem se naklonila a dala nad ně ruku , ale fakt to tam fest sálalo a já byla snad poprvý v životě na tátu pyšná .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['No , co si myslíte , když jsme se o dva dny pozdějc vraceli autem do Prahy a máma se nás zase pokoušela narvat pod asi deset tiráků , najednou tátovi řekla \" nemusel jsi mě tedy přede všemi ponížit \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Místo přitakání se máma při naprosto nesmyslném , sebe - vražedném a nás - vražedném předjížděcím manévru řítila přímo na protijedoucí VW transportér , přidávala plyn a v poslední chvíli vůz strhla tak , že nebýt Síly Na Její Straně , fakt to nemůžu jinak vyjádřit , bylo po nás i po těch Němcích , co frčeli proti nám v dobré víře , že neříděj v blázinci .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Když táta přešel lože ze žhavých uhlíků , rozhlídla jsem se a viděla , že si u těch lidí okolo , u tý cizí rodiny , kam se přiženil jako manžel č . 2 po zemřelým , jako náhražka , a nepřinesl žádnej statek a nezplodil s mámou žádný další potomky a nikdy nikoho ničím neoslnil , že si u nich poprvé v životě - a zřejmě taky naposledy - vysloužil když ne úctu , tak aspoň obdiv .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Nečekali za prvé , že to přejde , a za druhé , že si dovolí předběhnout rodinnou kněžku Imunity .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Já to najednou pochopila , uvědomila jsem si , co asi táta dělá , bleskově jsem se zula , stáhla jsem si punčocháče a vrazila na uhlí krátce po něm . Máma se tvářila jak obloha dvě vteřiny před krupobitím .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Po mně , a fakt jsem k němu poprvé po těch našich třech měsících chození cejtila něco jako lásku , šel bosej Péťa .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Ještě o něco pomalejc než táta a já , úplně v klidu , frajer .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Až pár tejdnů po tom vypečeným Štědrým dnu v Keplech mi vysvětlil , proč se tak klidně promenoval po žhavým uhlí .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Další moment , lidská kůže je taky dost špatnej vodič tepla .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Pokud teda nějakej zlomyslník do uhlíků nehodí třeba zátku od piva .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Neskutečně chytrej týpek , no ne ?', 'biased'], dtype='<U588'),\n",
       " array(['Přitom , jak asi chápete , celý aranžmá bylo , aniž to ovšem komukoliv řekla , že půjde první , jako šel první Mojžíš skrz Rudý moře .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Moc nevím , kdo potom přes uhlíky ještě šel a kdo si netrouf , stáhla jsem se s Péťou a tátou do jídelny , vytvořili jsme takový divný , tajně očkovaný společenství , ostatní zůstali s guru mámou ve stodole , a táta se mě najednou zeptal \" Mony , \" on ví , jak nesnáším Monču a Mončičáka , i když jsem mu to nikdy neřekla , prostě to poznal , \" Mony , Petr to ví ? \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Nevěděla jsem hned , co jako jestli Péťa ví , ale táta nečekal , až mi to docvakne , a řek Péťovi \" aby sis nemyslel , že je to tu samej cvok , já jsem očkovanej , jen to tu nikdo neví \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['No , nebyla jsem si úplně sure , jestli ta věta spíš neznamená , že to tu naopak je samej blázen včetně nás dvou , ale jednak jsme byli už dost vylitý , on ten punč fakt píše , a jednak se mi líbilo , jak mezi náma najednou vzniklo takový úplně jiný pouto , pouto tajný antisekty uprostřed sekty .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Jestli fakt o něco nestojím , tak se svlíkat před strejdou č . 1 a 2 a před jejich synama a dcerama , fakt díky , ale ne .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Copak nejsi ráda , že mi nic není ?', 'biased'], dtype='<U588'),\n",
       " array(['Okolo půlnoci začínali někteří členové naší slavný famílije vypadat hodně divně .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Ze svýho novýho džíbíelka a mobilu začala pouštět fakt dřevní muziku , hudbu Fra Fra , pokud to znáte , Funeral Songs , to je asi každýmu jasný , Naked , jako že na tenhle svět přijdeš a z tohodle světa odejdeš s ničím , Destiny a další pecky .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Teda ne že by zpívali anglicky , já si to pak doma všecko vygooglila .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Fra Fra zpívali nějakým africkým domorodým jazykem a působilo to neuvěřitelně původně , jako bysme se fakt přenesli o tři tisíce let dozadu .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Na mě i na Péťu , jak jsme byli velmi intenzivně zpunčovaný , vylitý jak džbány , to fungovalo úplně šíleně , trsali jsme oba v transu , okolo nás trsalo pár dalších , ale málo , málo lidí , taky máma , jenže já viděla , že se s ní něco děje .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Najednou mně jí bylo líto , trsali jsme s Péťou k ní a já se ptám \" mami , je ti něco \" a ona tak divně \" je mi , je mi \" , což jsem nechápala .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['My s Péťou tančili , já nevím , snad dvě hodiny , furt na Fra Fra , furt dokola , ale já si všimla , že okolo nás už nikdo , jen máma , tančilo se v jídelně , stoly srazili do kouta .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Najednou máma klesla na kolena a začala dávit , běželi jsme jí pomoct , můj medik na ni mluvil , jenže ona že musí na záchod , je mi to blbý popisovat , muselo to bejt fakt děsný , protože to evidentně neudržela , jestli chápete , no a my ji táhli k wécé , máme na statku dva záchody , jenže wécé byly obsazený a před nima stáli čekající v křečích a zevnitř byl slyšet nářek , tak jsme mámu táhli ven , visela na nás jak hadr , jenomže na dvoře další lidi a všichni ve dřepu a všichni vzdychali a všude bylo vidět , že to z nich lítá ven .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Rozpoutalo se průjmový peklo , všichni , úplně všichni to chytli , jen já , Péťa a táta nic .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Nechápu , jestli to bylo tím tajným očkováním , nevím , ale naše rodina tam s prominutím srala celej další den , probíhala permanentní rvačka o záchody a o dva plechový kýbly .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Jak vypadal náš dvůr , to se mi ani nechce popisovat , strašný , Péťa s mým tátou to odváželi smíchaný se sněhem v kolečkách ven za statek , tam , co je to velký hnojiště .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Uvnitř to vypadalo jak ve válečným lazaretu , já vzala auto a jela do Sušice shánět endiaron , ale když jsem se za dvě hodiny vrátila s několika krabičkama , tak na mě máma ječela , že ona to laktobacily vraždící svinstvo jíst nebude , má imunitu , už to ale vyslovovala s malým i , a její tělo si s tím \" musí poradit , musí \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Bylo jí divný , že já tu dysentérii nebo co to bylo nemám .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['\" Jak je to možný , \" pátrala , \" zrovna ty , u který bych teda žádnou odolnost rozhodně nečekala . \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Někdy jsem fakt blbá , a tak jsem si bohužel neodpustila poznámku \" asi mám lepší imunitu \" a ona jak to prej myslím .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Zřejmě za to tak trochu mohl Péťa , protože mi řekl , že jedním z projevů covidu může bejt i průjem , přitom bylo jasný , že to na Keply zavlekl ten malej , tříletej mazel .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Chápete to naprosto správně , tátu jsem vynechala , táta se bál , že na něj bude máma guru naštvaná , takže předstíral , jako že má taky průjem , dělal , že lítá jako ostatní , dokonce dělal , jako že si pere spodky v koupelně , prostě jel na jistotu a hrál ten tyátr se vším všudy .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Když uslyšela , co jsem řekla , zasyčela \" takže ty jsi zrádce \" a pak ještě \" no jasně , proč mě to vůbec nepřekvapuje \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Byl u toho i Evžen Bavorák , ten její zazobanej šamstr , a najednou povídá \" taky se nechám naočkovat , dyť tohle je hotový peklo a jim nic není \" , za což okamžitě dostal ale neskutečnej kartáč .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['No nic , na ségře mi nesejde , stejně mě nesnáší od minuty , kdy jsem se narodila , jenomže Hedvika to samo sebou práskla mámě .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['A bylo zle .', 'biased'], dtype='<U588'),\n",
       " array(['Máma mi řekla pouze toto : \" No to jsi mne , Moničko , strašlivě zklamala , to jsi zklamala celou naši rodinu , nic horšího jsi mi asi nemohla provést . \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Řekla jsem \" ale mami , copak nejsi ráda , že mi nic není , že nemám ten pekelnej průjem , že mi neselhala imunita ? \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Zesláblým hlasem na mě křičela : \" Ty nemluv o imunitě , ty sis to svinstvo nechala dát do těla , strašně , straně jsi nás podrazila . \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Od toho Štědrýho večera a následující noci na mě máma guru nepromluvila .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Sice pořád skoro nic neříká , hodiny v klidu promlčí , ale je fakt milej .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Nebejt táty , kterýho odmítám nechat s guru mámou samotnýho , určitě bych se k Péťovi okamžitě odstěhovala , nabízí mi to každej den , co jsme se vrátili z Kepel .',\n",
       "        'biased'], dtype='<U588')]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x : x[1] == 'biased',annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f9137e02-24e8-45e1-a7f4-bfd99e9c190f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3436089f-506a-43e6-be0c-619870db1cae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
